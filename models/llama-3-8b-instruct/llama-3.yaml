experiment:
  name: llama-3-8b-instruct

model:
  model: llama-3-8b-instruct
  type: pretrained_decoder
  attn_implementation: env(ATTN_IMPLEMENTATION:flash_attention_2)

seed: 22

inference:
  max_length: 8192
  window:
    type: full
  tokenizer: file(../../configs/tokenizers/llama-3.yaml)
  eos: <|eot_id|>
  chat_template: file(../../configs/chat_templates/llama-3.yaml)
