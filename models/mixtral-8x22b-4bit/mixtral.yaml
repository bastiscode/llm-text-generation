experiment:
  name: mixtral-8x22b-4bit
input_tokenizer:
  tokenize:
    type: huggingface
    path: relpath(mixtral/tokenizer.json)
  special:
    tokens:
      - <unk>
    pad: <unk>
  eos_token: </s>
model:
  name: mixtral-8x22b-4bit
  type: pretrained_decoder
  attn_implementation: env(MIXTRAL_ATTN_IMPLEMENTATION:null)
  device_map:
    model.embed_tokens: 0
    model.layers.{0..8}: 0
    model.layers.{9..21}: 1
    model.layers.{22..34}: 2
    model.layers.{35..47}: 3
    model.layers.{48..55}: 4
    model.norm: 4
    lm_head: 4
seed: 22
train:
  data:
    max_length: 65536
