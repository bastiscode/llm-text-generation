experiment:
  name: mixtral-8x22b-4bit

model:
  model: mixtral-8x22b-4bit
  type: pretrained_decoder
  attn_implementation: env(ATTN_IMPLEMENTATION:null)
  device_map:
    model.embed_tokens: 0
    model.layers.{0..8}: 0
    model.layers.{9..21}: 1
    model.layers.{22..34}: 2
    model.layers.{35..47}: 3
    model.layers.{48..55}: 4
    model.norm: 4
    lm_head: 4

seed: 22

inference:
  max_length: 65536
  window:
    type: full
  tokenizer: file(../../configs/tokenizers/mistral.yaml)
  eos: </s>
